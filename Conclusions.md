Big Data Machine Learning poses many additional challenges not encountered in traditional-scale ML such as I/O and serial compute bottleneck as well as memory and cache management when handling large datasets. We have tackled many of the distributed machine learning challenges in this project. We deployed an elastic Hadoop YARN cluster with Spark to achieve efficient scaling and lower execution time while maintaining similar accuracy.

The primary area for improvement we suggest is to reduce the communication overhead by more efficiently communicating model parameters. Currently we run a Flask server on the master node, but many other methods such as MPI ring-AllReduce have been proposed.4 Further improvements include adjustment of the Elephas package to include multi-level model inputs allowing us to train a bi-LSTM with frame-level pixel and audio features. Additionally, to achieve higher accuracy, it will be helpful to utilize more feature channels and the full version (1.5TB) of this dataset.
